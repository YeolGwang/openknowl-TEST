{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"21. yolov3_training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpvZnHe4OeLv","executionInfo":{"status":"ok","timestamp":1659840615313,"user_tz":-540,"elapsed":19057,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"6f071d6a-dd5d-4ba7-8c84-2e277e2706d6"},"source":["import os, sys \n","from google.colab import drive \n","\n","drive.mount('./content')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at ./content\n"]}]},{"cell_type":"code","metadata":{"id":"QKw7HZG7LgtP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659840615314,"user_tz":-540,"elapsed":19,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"60116c60-7381-4ed2-8157-281003d1fbec"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mcontent\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INA85-DyYBEE","executionInfo":{"status":"ok","timestamp":1659840615315,"user_tz":-540,"elapsed":12,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"8935cf9a-87fb-40b1-f6d6-251678141d7d"},"source":["cd /content/content/MyDrive/openknowl/codes"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/content/MyDrive/openknowl/codes\n"]}]},{"cell_type":"markdown","metadata":{"id":"9ISd6eTgXQNd"},"source":["config"]},{"cell_type":"code","metadata":{"id":"wQzbsaxpXSVs"},"source":["# YOLO options\n","YOLO_DARKNET_WEIGHTS        = \"./model_data/yolov3.weights\"\n","YOLO_COCO_CLASSES           = \"./model_data/coco.names\"\n","YOLO_STRIDES                = [8, 16, 32]\n","YOLO_IOU_LOSS_THRESH        = 0.5\n","YOLO_ANCHOR_PER_SCALE       = 3\n","YOLO_MAX_BBOX_PER_SCALE     = 100\n","YOLO_INPUT_SIZE             = 416\n","YOLO_ANCHORS                = [[[10,  13], [16,   30], [33,   23]],\n","                               [[30,  61], [62,   45], [59,  119]],\n","                               [[116, 90], [156, 198], [373, 326]]]\n","# Train options\n","TRAIN_CLASSES               = \"./racoon_data/racoon.names\"\n","TRAIN_ANNOT_PATH            = \"./racoon_data/racoon_train.txt\"\n","TRAIN_BATCH_SIZE            = 4\n","TRAIN_INPUT_SIZE            = 416\n","TRAIN_DATA_AUG              = True\n","TRAIN_TRANSFER              = False\n","TRAIN_FROM_CHECKPOINT       = \"./checkpoints/yolov3_custom\"\n","\n","TRAIN_LR_INIT               = 1e-4\n","TRAIN_LR_END                = 1e-6\n","TRAIN_WARMUP_EPOCHS         = 2\n","TRAIN_EPOCHS                = 30\n","\n","# TEST options\n","TEST_ANNOT_PATH             = \"./racoon_data/racoon_test.txt\"\n","TEST_BATCH_SIZE             = 4\n","TEST_INPUT_SIZE             = 416\n","TEST_DATA_AUG               = False\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KzpkCyp95zC"},"source":["import cv2\n","import time\n","import random\n","import colorsys\n","import numpy as np\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8JMPV9_-m1M"},"source":["utils"]},{"cell_type":"code","metadata":{"id":"raaIsjRk98PJ"},"source":["def load_yolo_weights(model, weights_file):\n","    tf.keras.backend.clear_session() # used to reset layer names\n","    # load Darknet original weights to Keras model\n","    with open(weights_file, 'rb') as wf:\n","        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n","\n","        j = 0\n","        for i in range(75):\n","            if i > 0:\n","                conv_layer_name = 'conv2d_%d' %i\n","            else:\n","                conv_layer_name = 'conv2d'\n","                \n","            if j > 0:\n","                bn_layer_name = 'batch_normalization_%d' %j\n","            else:\n","                bn_layer_name = 'batch_normalization'\n","            \n","            conv_layer = model.get_layer(conv_layer_name)\n","            filters = conv_layer.filters\n","            k_size = conv_layer.kernel_size[0]\n","            in_dim = conv_layer.input_shape[-1]\n","\n","            if i not in [58, 66, 74]:\n","                # darknet weights: [beta, gamma, mean, variance]\n","                bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n","                # tf weights: [gamma, beta, mean, variance]\n","                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n","                bn_layer = model.get_layer(bn_layer_name)\n","                j += 1\n","            else:\n","                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n","\n","            # darknet shape (out_dim, in_dim, height, width)\n","            conv_shape = (filters, in_dim, k_size, k_size)\n","            conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n","            # tf shape (height, width, in_dim, out_dim)\n","            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n","\n","            if i not in [58, 66, 74]:\n","                conv_layer.set_weights([conv_weights])\n","                bn_layer.set_weights(bn_weights)\n","            else:\n","                conv_layer.set_weights([conv_weights, conv_bias])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBrLfn24-CaP"},"source":["def read_class_names(class_file_name):\n","    # loads class name from a file\n","    #print(class_file_name)\n","    names = {}\n","    with open(class_file_name, 'r') as data:\n","        for ID, name in enumerate(data):\n","            names[ID] = name.strip('\\n')\n","    return names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IKBK700-EXF"},"source":["def image_preprocess(image, target_size, gt_boxes=None):\n","    ih, iw    = target_size\n","    h,  w, _  = image.shape\n","\n","    scale = min(iw/w, ih/h)\n","    nw, nh  = int(scale * w), int(scale * h)\n","    image_resized = cv2.resize(image, (nw, nh))\n","\n","    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n","    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n","    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n","    image_paded = image_paded / 255.\n","\n","    if gt_boxes is None:\n","        return image_paded\n","\n","    else:\n","        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n","        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n","        return image_paded, gt_boxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDX_984L-F1Y"},"source":["\n","def draw_bbox(image, bboxes, CLASSES=YOLO_COCO_CLASSES, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors=''):   \n","    NUM_CLASS = read_class_names(CLASSES)\n","    num_classes = len(NUM_CLASS)\n","    image_h, image_w, _ = image.shape\n","    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n","    #print(\"hsv_tuples\", hsv_tuples)\n","    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n","    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n","\n","    random.seed(0)\n","    random.shuffle(colors)\n","    random.seed(None)\n","\n","    for i, bbox in enumerate(bboxes):\n","        coor = np.array(bbox[:4], dtype=np.int32)\n","        score = bbox[4]\n","        class_ind = int(bbox[5])\n","        bbox_color = rectangle_colors if rectangle_colors != ''else colors[class_ind]\n","        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n","        if bbox_thick < 1: bbox_thick = 1\n","        #print(image_h, image_w, bbox_thick)\n","        fontScale = 0.75 * bbox_thick\n","        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n","\n","        # put object rectangle\n","        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n","\n","        if show_label:\n","            # get text label\n","            score_str = f' {score:.2f}' if show_confidence else '' \n","            label = f'{NUM_CLASS[class_ind]}' + score_str\n","\n","            # get text size\n","            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n","                                                                  fontScale, thickness=bbox_thick)\n","            # put filled text rectangle\n","            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n","\n","            # put text above rectangle\n","            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n","                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n","\n","                        \n","\n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HY2vi4Fi-OsJ"},"source":["def bboxes_iou(boxes1, boxes2):\n","    boxes1 = np.array(boxes1)\n","    boxes2 = np.array(boxes2)\n","\n","    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n","    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n","\n","    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n","    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n","\n","    inter_section = np.maximum(right_down - left_up, 0.0)\n","    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n","    union_area    = boxes1_area + boxes2_area - inter_area\n","    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n","\n","    return ious\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxduNYb6-QNh"},"source":["\n","def nms(bboxes, iou_threshold, sigma=0.3):\n","\n","    classes_in_img = list(set(bboxes[:, 5]))\n","    best_bboxes = []\n","\n","    for cls in classes_in_img:\n","        cls_mask = (bboxes[:, 5] == cls)\n","        cls_bboxes = bboxes[cls_mask]\n","        # Process 1: Determine whether the number of bounding boxes is greater than 0 \n","        while len(cls_bboxes) > 0:\n","            # Process 2: Select the bounding box with the highest score according to socre order A\n","            max_ind = np.argmax(cls_bboxes[:, 4])\n","            best_bbox = cls_bboxes[max_ind]\n","            best_bboxes.append(best_bbox)\n","            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n","            # Process 3: Calculate this bounding box A and\n","            # Remain all iou of the bounding box and remove those bounding boxes whose iou value is higher than the threshold \n","            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n","            weight = np.ones((len(iou),), dtype=np.float32)\n","\n","            iou_mask = iou > iou_threshold\n","            weight[iou_mask] = 0.0\n","\n","            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n","            score_mask = cls_bboxes[:, 4] > 0.\n","            cls_bboxes = cls_bboxes[score_mask]\n","\n","    return best_bboxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcenMeqH-SeO"},"source":["def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):\n","    valid_scale=[0, np.inf]\n","    pred_bbox = np.array(pred_bbox)\n","\n","    pred_xywh = pred_bbox[:, 0:4]\n","    pred_conf = pred_bbox[:, 4]\n","    pred_prob = pred_bbox[:, 5:]\n","\n","    # 1. (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n","    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n","                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n","    # 2. (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n","    org_h, org_w = original_image.shape[:2]\n","    resize_ratio = min(input_size / org_w, input_size / org_h)\n","\n","    dw = (input_size - resize_ratio * org_w) / 2\n","    dh = (input_size - resize_ratio * org_h) / 2\n","\n","    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n","    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n","\n","    # 3. clip some boxes those are out of range\n","    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n","                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n","    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n","    pred_coor[invalid_mask] = 0\n","\n","    # 4. discard some invalid boxes\n","    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n","    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n","\n","    # 5. discard boxes with low scores\n","    classes = np.argmax(pred_prob, axis=-1)\n","    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n","    score_mask = scores > score_threshold\n","    mask = np.logical_and(scale_mask, score_mask)\n","    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n","    #print(coors)\n","\n","    #print(coors[0])\n","    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wf91vRsKXTQd"},"source":["def detect_image(YoloV3, image_path, output_path, input_size=416, show=False, CLASSES=YOLO_COCO_CLASSES, score_threshold=0.3, iou_threshold=0.45, rectangle_colors=''):\n","    original_image      = cv2.imread(image_path)\n","\n","    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n","    image_data = tf.expand_dims(image_data, 0)\n","\n","    pred_bbox = YoloV3.predict(image_data)\n","    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n","    pred_bbox = tf.concat(pred_bbox, axis=0)\n","    \n","    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n","    #xx=np.array(bboxes)\n","    #print(xx.shape)\n","\n","    bboxes = nms(bboxes, iou_threshold)\n","    image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, rectangle_colors=rectangle_colors)\n","    #cv2.imwrite('./output.jpg', image)\n","        \n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VrWfbwjQ-fiU"},"source":["model"]},{"cell_type":"code","metadata":{"id":"YDDDzxjh-qmA"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, ZeroPadding2D, BatchNormalization\n","from tensorflow.keras.regularizers import l2\n","\n","\n","\n","STRIDES         = np.array(YOLO_STRIDES)\n","ANCHORS         = (np.array(YOLO_ANCHORS).T/STRIDES).T\n","IOU_LOSS_THRESH = YOLO_IOU_LOSS_THRESH\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMrWDDaDXaE_"},"source":["class BatchNormalization(BatchNormalization):\n","    # \"Frozen state\" and \"inference mode\" are two separate concepts.\n","    # `layer.trainable = False` is to freeze the layer, so the layer will use\n","    # stored moving `var` and `mean` in the \"inference mode\", and both `gama`\n","    # and `beta` will not be updated !\n","    def call(self, x, training=False):\n","        if not training:\n","            training = tf.constant(False)\n","        training = tf.logical_and(training, self.trainable)\n","        return super().call(x, training)\n","\n","#training :False inference => moving mean, True training = > mean\n","#trainable : False (frozen), Ture (Not frozen)\n","\n","def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):\n","    if downsample:\n","        input_layer = ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n","        padding = 'valid'\n","        strides = 2\n","    else:\n","        strides = 1\n","        padding = 'same'\n","\n","    conv = Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides,\n","                  padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),\n","                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n","                  bias_initializer=tf.constant_initializer(0.))(input_layer)\n","    if bn:\n","        conv = BatchNormalization()(conv)\n","    if activate == True:\n","        conv = LeakyReLU(alpha=0.1)(conv)\n","\n","    return conv\n","\n","def residual_block(input_layer, input_channel, filter_num1, filter_num2):\n","    short_cut = input_layer\n","    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))\n","    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))\n","\n","    residual_output = short_cut + conv\n","    return residual_output\n","\n","def upsample(input_layer):\n","    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')\n","\n","\n","def darknet53(input_data):\n","    input_data = convolutional(input_data, (3, 3,  3,  32))\n","    input_data = convolutional(input_data, (3, 3, 32,  64), downsample=True)\n","\n","    for i in range(1):\n","        input_data = residual_block(input_data,  64,  32, 64)\n","\n","    input_data = convolutional(input_data, (3, 3,  64, 128), downsample=True)\n","\n","    for i in range(2):\n","        input_data = residual_block(input_data, 128,  64, 128)\n","\n","    input_data = convolutional(input_data, (3, 3, 128, 256), downsample=True)\n","\n","    for i in range(8):\n","        input_data = residual_block(input_data, 256, 128, 256)\n","\n","    route_1 = input_data\n","    input_data = convolutional(input_data, (3, 3, 256, 512), downsample=True)\n","\n","    for i in range(8):\n","        input_data = residual_block(input_data, 512, 256, 512)\n","\n","    route_2 = input_data\n","    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True)\n","\n","    for i in range(4):\n","        input_data = residual_block(input_data, 1024, 512, 1024)\n","\n","    return route_1, route_2, input_data\n","\n","def YOLOv3(input_layer, NUM_CLASS):\n","    # After the input layer enters the Darknet-53 network, we get three branches\n","    route_1, route_2, conv = darknet53(input_layer)\n","    # See the orange module (DBL) in the figure above, a total of 5 Subconvolution operation\n","    conv = convolutional(conv, (1, 1, 1024,  512))\n","    conv = convolutional(conv, (3, 3,  512, 1024))\n","    conv = convolutional(conv, (1, 1, 1024,  512))\n","    conv = convolutional(conv, (3, 3,  512, 1024))\n","    conv = convolutional(conv, (1, 1, 1024,  512))\n","    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))\n","    \n","    # conv_lbbox is used to predict large-sized objects , Shape = [None, 13, 13, 255] \n","    conv_lbbox = convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n","\n","    conv = convolutional(conv, (1, 1,  512,  256))\n","    # upsample here uses the nearest neighbor interpolation method, which has the advantage that the\n","    # upsampling process does not need to learn, thereby reducing the network parameter  \n","    conv = upsample(conv)\n","\n","    conv = tf.concat([conv, route_2], axis=-1)\n","    conv = convolutional(conv, (1, 1, 768, 256))\n","    conv = convolutional(conv, (3, 3, 256, 512))\n","    conv = convolutional(conv, (1, 1, 512, 256))\n","    conv = convolutional(conv, (3, 3, 256, 512))\n","    conv = convolutional(conv, (1, 1, 512, 256))\n","    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))\n","\n","    # conv_mbbox is used to predict medium-sized objects, shape = [None, 26, 26, 255]\n","    conv_mbbox = convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n","\n","    conv = convolutional(conv, (1, 1, 256, 128))\n","    conv = upsample(conv)\n","\n","    conv = tf.concat([conv, route_1], axis=-1)\n","    conv = convolutional(conv, (1, 1, 384, 128))\n","    conv = convolutional(conv, (3, 3, 128, 256))\n","    conv = convolutional(conv, (1, 1, 256, 128))\n","    conv = convolutional(conv, (3, 3, 128, 256))\n","    conv = convolutional(conv, (1, 1, 256, 128))\n","    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))\n","    \n","    # conv_sbbox is used to predict small size objects, shape = [None, 52, 52, 255]\n","    conv_sbbox = convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)\n","        \n","    return [conv_sbbox, conv_mbbox, conv_lbbox]\n","\n","def Create_Yolov3(input_size=416, channels=3, training=False, CLASSES=YOLO_COCO_CLASSES):\n","    NUM_CLASS = len(read_class_names(CLASSES))\n","    input_layer  = Input([input_size, input_size, channels])\n","\n","    conv_tensors = YOLOv3(input_layer, NUM_CLASS)\n","\n","    output_tensors = []\n","    for i, conv_tensor in enumerate(conv_tensors):\n","        pred_tensor = decode(conv_tensor, NUM_CLASS, i)\n","        if training: output_tensors.append(conv_tensor)\n","        output_tensors.append(pred_tensor)\n","\n","    YoloV3 = tf.keras.Model(input_layer, output_tensors)\n","    return YoloV3\n","\n","def decode(conv_output, NUM_CLASS, i=0):\n","    # where i = 0, 1 or 2 to correspond to the three grid scales  \n","    conv_shape       = tf.shape(conv_output)\n","    batch_size       = conv_shape[0]\n","    output_size      = conv_shape[1]\n","\n","    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n","\n","    conv_raw_dxdy = conv_output[:, :, :, :, 0:2] # offset of center position     \n","    conv_raw_dwdh = conv_output[:, :, :, :, 2:4] # Prediction box length and width offset\n","    conv_raw_conf = conv_output[:, :, :, :, 4:5] # confidence of the prediction box\n","    conv_raw_prob = conv_output[:, :, :, :, 5: ] # category probability of the prediction box \n","\n","    # next need Draw the grid. Where output_size is equal to 13, 26 or 52  \n","    y = tf.range(output_size, dtype=tf.int32)\n","    y = tf.expand_dims(y, -1)\n","    y = tf.tile(y, [1, output_size])\n","    x = tf.range(output_size,dtype=tf.int32)\n","    x = tf.expand_dims(x, 0)\n","    x = tf.tile(x, [output_size, 1])\n","\n","    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n","    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\n","    xy_grid = tf.cast(xy_grid, tf.float32)\n","\n","    # Calculate the center position of the prediction box:\n","    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n","    # Calculate the length and width of the prediction box:\n","    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n","\n","    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n","    pred_conf = tf.sigmoid(conv_raw_conf) # object box calculates the predicted confidence\n","    pred_prob = tf.sigmoid(conv_raw_prob) # calculating the predicted probability category box object\n","\n","    # calculating the predicted probability category box object\n","    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n","\n","def bbox_iou(boxes1, boxes2):\n","    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n","    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n","\n","    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n","                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n","    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n","                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n","\n","    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n","    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n","\n","    inter_section = tf.maximum(right_down - left_up, 0.0)\n","    inter_area = inter_section[..., 0] * inter_section[..., 1]\n","    union_area = boxes1_area + boxes2_area - inter_area\n","\n","    return 1.0 * inter_area / union_area\n","\n","def bbox_giou(boxes1, boxes2):\n","    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n","                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n","    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n","                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n","\n","    boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\n","                        tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\n","    boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\n","                        tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\n","\n","    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n","    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n","\n","    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n","    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n","\n","    inter_section = tf.maximum(right_down - left_up, 0.0)\n","    inter_area = inter_section[..., 0] * inter_section[..., 1]\n","    union_area = boxes1_area + boxes2_area - inter_area\n","    iou = inter_area / union_area\n","\n","    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n","    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n","    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\n","    enclose_area = enclose[..., 0] * enclose[..., 1]\n","    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\n","\n","    return giou\n","\n","\n","def compute_loss(pred, conv, label, bboxes, i=0, CLASSES=YOLO_COCO_CLASSES):\n","    NUM_CLASS = len(read_class_names(CLASSES))\n","    conv_shape  = tf.shape(conv)\n","    batch_size  = conv_shape[0]\n","    output_size = conv_shape[1]\n","    input_size  = STRIDES[i] * output_size\n","    conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n","\n","    conv_raw_conf = conv[:, :, :, :, 4:5]\n","    conv_raw_prob = conv[:, :, :, :, 5:]\n","\n","    pred_xywh     = pred[:, :, :, :, 0:4]\n","    pred_conf     = pred[:, :, :, :, 4:5]\n","\n","    label_xywh    = label[:, :, :, :, 0:4]\n","    respond_bbox  = label[:, :, :, :, 4:5]\n","    label_prob    = label[:, :, :, :, 5:]\n","\n","    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\n","    input_size = tf.cast(input_size, tf.float32)\n","\n","    #https://giou.stanford.edu/GIoU.pdf\n","    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n","    giou_loss = respond_bbox * bbox_loss_scale * (1- giou)\n","\n","    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\n","    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\n","\n","    # respond_bbox 1인경우, bgd = 0 ; rbbox가 0인동시에 max iou < 0.5인경우 bgd = 1\n","    respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32 )\n","\n","    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n","\n","    conf_loss = conf_focal * (\n","            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n","            +\n","            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n","    )\n","\n","    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n","\n","    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\n","    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\n","    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\n","\n","    return giou_loss, conf_loss, prob_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vD29zVTnjHps","executionInfo":{"status":"ok","timestamp":1659840775433,"user_tz":-540,"elapsed":298,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"f8669b85-1d9d-421b-98a8-0456f79117f4"},"source":["  y = tf.range(13, dtype=tf.int32)\n","  y = tf.expand_dims(y, -1)\n","  y = tf.tile(y, [1, 13])\n","  y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(13, 13), dtype=int32, numpy=\n","array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n","       [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n","       [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n","       [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n","       [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n","       [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n","       [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n","       [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n","       [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n","       [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n","       [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n","       [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]], dtype=int32)>"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"OoTCdT7V-isD"},"source":["dataset "]},{"cell_type":"code","metadata":{"id":"P9FsKyf7Xdfu"},"source":["\n","import os\n","import cv2\n","import random\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","\n","class Dataset(object):\n","    # Dataset preprocess implementation\n","    def __init__(self, dataset_type):\n","        self.annot_path  = TRAIN_ANNOT_PATH if dataset_type == 'train' else TEST_ANNOT_PATH\n","        self.input_sizes = TRAIN_INPUT_SIZE if dataset_type == 'train' else TEST_INPUT_SIZE\n","        self.batch_size  = TRAIN_BATCH_SIZE if dataset_type == 'train' else TEST_BATCH_SIZE\n","        self.data_aug    = TRAIN_DATA_AUG   if dataset_type == 'train' else TEST_DATA_AUG\n","\n","        self.train_input_sizes = TRAIN_INPUT_SIZE\n","        self.strides = np.array(YOLO_STRIDES)\n","        self.classes = read_class_names(TRAIN_CLASSES)\n","        self.num_classes = len(self.classes)\n","        self.anchors = (np.array(YOLO_ANCHORS).T/self.strides).T\n","        self.anchor_per_scale = YOLO_ANCHOR_PER_SCALE\n","        self.max_bbox_per_scale = YOLO_MAX_BBOX_PER_SCALE\n","\n","        self.annotations = self.load_annotations(dataset_type)\n","        self.num_samples = len(self.annotations)\n","        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))\n","        self.batch_count = 0\n","\n","\n","    def load_annotations(self, dataset_type):\n","        final_annotations = []\n","        with open(self.annot_path, 'r') as f:\n","            txt = f.readlines()\n","            annotations = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n","        np.random.shuffle(annotations)\n","        \n","        for annotation in annotations:\n","            # fully parse annotations\n","            line = annotation.split()\n","            image_path, index = \"\", 1\n","            for i, one_line in enumerate(line):\n","                if not one_line.replace(\",\",\"\").isnumeric():\n","                    if image_path != \"\": image_path += \" \"\n","                    image_path += one_line\n","                else:\n","                    index = i\n","                    break\n","            if not os.path.exists(image_path):\n","                raise KeyError(\"%s does not exist ... \" %image_path)\n","\n","            final_annotations.append([image_path, line[index:]])\n","\n","        return final_annotations\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        with tf.device('/cpu:0'):\n","            self.train_input_size = random.choice([self.train_input_sizes])\n","            self.train_output_sizes = self.train_input_size // self.strides\n","\n","            batch_image = np.zeros((self.batch_size, self.train_input_size, self.train_input_size, 3), dtype=np.float32)\n","\n","            batch_label_sbbox = np.zeros((self.batch_size, self.train_output_sizes[0], self.train_output_sizes[0],\n","                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n","            batch_label_mbbox = np.zeros((self.batch_size, self.train_output_sizes[1], self.train_output_sizes[1],\n","                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n","            batch_label_lbbox = np.zeros((self.batch_size, self.train_output_sizes[2], self.train_output_sizes[2],\n","                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n","\n","            batch_sbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n","            batch_mbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n","            batch_lbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n","\n","            num = 0\n","            if self.batch_count < self.num_batchs:\n","                while num < self.batch_size:\n","                    index = self.batch_count * self.batch_size + num\n","                    if index >= self.num_samples: index -= self.num_samples\n","                    annotation = self.annotations[index]\n","                    image, bboxes = self.parse_annotation(annotation)\n","                    label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)\n","\n","                    batch_image[num, :, :, :] = image\n","                    batch_label_sbbox[num, :, :, :, :] = label_sbbox\n","                    batch_label_mbbox[num, :, :, :, :] = label_mbbox\n","                    batch_label_lbbox[num, :, :, :, :] = label_lbbox\n","                    batch_sbboxes[num, :, :] = sbboxes\n","                    batch_mbboxes[num, :, :] = mbboxes\n","                    batch_lbboxes[num, :, :] = lbboxes\n","                    num += 1\n","                self.batch_count += 1\n","                batch_smaller_target = batch_label_sbbox, batch_sbboxes\n","                batch_medium_target  = batch_label_mbbox, batch_mbboxes\n","                batch_larger_target  = batch_label_lbbox, batch_lbboxes\n","\n","                return batch_image, (batch_smaller_target, batch_medium_target, batch_larger_target)\n","            else:\n","                self.batch_count = 0\n","                np.random.shuffle(self.annotations)\n","                raise StopIteration\n","\n","    def random_horizontal_flip(self, image, bboxes):\n","        if random.random() < 0.5:\n","            _, w, _ = image.shape\n","            image = image[:, ::-1, :]\n","            bboxes[:, [0,2]] = w - bboxes[:, [2,0]]\n","\n","        return image, bboxes\n","\n","    def random_crop(self, image, bboxes):\n","        if random.random() < 0.5:\n","            h, w, _ = image.shape\n","            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n","\n","            max_l_trans = max_bbox[0]\n","            max_u_trans = max_bbox[1]\n","            max_r_trans = w - max_bbox[2]\n","            max_d_trans = h - max_bbox[3]\n","\n","            crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n","            crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n","            crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n","            crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n","\n","            image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]\n","\n","            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n","            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n","\n","        return image, bboxes\n","\n","    def random_translate(self, image, bboxes):\n","        if random.random() < 0.5:\n","            h, w, _ = image.shape\n","            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n","\n","            max_l_trans = max_bbox[0]\n","            max_u_trans = max_bbox[1]\n","            max_r_trans = w - max_bbox[2]\n","            max_d_trans = h - max_bbox[3]\n","\n","            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n","            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n","\n","            M = np.array([[1, 0, tx], [0, 1, ty]])\n","            image = cv2.warpAffine(image, M, (w, h))\n","\n","            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n","            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n","\n","        return image, bboxes\n","\n","    def parse_annotation(self, annotation):\n","\n","        image_path = annotation[0]\n","        image = cv2.imread(image_path)\n","            \n","        bboxes = np.array([list(map(int, box.split(','))) for box in annotation[1]])\n","\n","        if self.data_aug:\n","            image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\n","            image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\n","            image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\n","\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image, bboxes = image_preprocess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\n","        return image, bboxes\n","\n","    def preprocess_true_boxes(self, bboxes):\n","        label = [np.zeros((self.train_output_sizes[i], self.train_output_sizes[i], self.anchor_per_scale,\n","                           5 + self.num_classes)) for i in range(3)]\n","        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\n","        bbox_count = np.zeros((3,))\n","\n","        for bbox in bboxes:\n","            bbox_coor = bbox[:4]\n","            bbox_class_ind = bbox[4]\n","\n","            onehot = np.zeros(self.num_classes, dtype=np.float)\n","            onehot[bbox_class_ind] = 1.0\n","            uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)\n","            deta = 0.01\n","            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\n","\n","            bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n","            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\n","\n","            iou = []\n","            exist_positive = False\n","            for i in range(3):\n","                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n","                anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n","                anchors_xywh[:, 2:4] = self.anchors[i]\n","\n","                iou_scale = bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\n","                iou.append(iou_scale)\n","                iou_mask = iou_scale > 0.3\n","\n","                if np.any(iou_mask):\n","                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n","\n","                    label[i][yind, xind, iou_mask, :] = 0\n","                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n","                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n","                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n","\n","                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\n","                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n","                    bbox_count[i] += 1\n","\n","                    exist_positive = True\n","\n","            if not exist_positive:\n","                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n","                best_detect = int(best_anchor_ind / self.anchor_per_scale)\n","                best_anchor = int(best_anchor_ind % self.anchor_per_scale)\n","                xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n","\n","                label[best_detect][yind, xind, best_anchor, :] = 0\n","                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n","                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n","                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n","                \n","                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\n","                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n","                bbox_count[best_detect] += 1\n","\n","        label_sbbox, label_mbbox, label_lbbox = label\n","        sbboxes, mbboxes, lbboxes = bboxes_xywh\n","        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n","\n","    def __len__(self):\n","        return self.num_batchs\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dzO1B3OScgLb"},"source":["https://www.tensorflow.org/guide/autodiff"]},{"cell_type":"markdown","metadata":{"id":"kIQuRMfmXpgX"},"source":["training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"eMp-XBHSXicX","executionInfo":{"status":"error","timestamp":1659840785503,"user_tz":-540,"elapsed":3150,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"b7f2b46d-cb3b-445f-a403-dbe906984507"},"source":["\n","\n","import os\n","import shutil\n","import numpy as np\n","import tensorflow as tf\n","\n","global TRAIN_FROM_CHECKPOINT\n","input_size = YOLO_INPUT_SIZE\n","Darknet_weights = YOLO_DARKNET_WEIGHTS\n","\n","\n","save_best_only = True # saves only best model according validation loss\n","save_checkpoints = False # saves all best validated checkpoints in training process (may require a lot disk space)\n","\n","\n","trainset = Dataset('train')\n","testset = Dataset('test')\n","\n","steps_per_epoch = len(trainset)\n","global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n","warmup_steps = TRAIN_WARMUP_EPOCHS * steps_per_epoch\n","total_steps = TRAIN_EPOCHS * steps_per_epoch\n","\n","if TRAIN_TRANSFER:\n","    Darknet = Create_Yolov3(input_size=input_size)\n","    load_yolo_weights(Darknet, Darknet_weights) # use darknet weights\n","\n","yolo = Create_Yolov3(input_size=input_size, training=True, CLASSES=TRAIN_CLASSES)\n","\n","\n","if TRAIN_FROM_CHECKPOINT:\n","    yolo.load_weights(TRAIN_FROM_CHECKPOINT)\n","\n","\n","optimizer = tf.keras.optimizers.Adam()\n","\n","def train_step(image_data, target):\n","    with tf.GradientTape() as tape:\n","        pred_result = yolo(image_data, training=True)\n","        giou_loss=conf_loss=prob_loss=0\n","        # optimizing process\n","        for i in range(3):\n","            conv, pred = pred_result[i*2], pred_result[i*2+1]\n","            loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=TRAIN_CLASSES)\n","            giou_loss += loss_items[0]\n","            conf_loss += loss_items[1]\n","            prob_loss += loss_items[2]\n","        total_loss = giou_loss + conf_loss + prob_loss\n","        gradients = tape.gradient(total_loss, yolo.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, yolo.trainable_variables))\n","\n","\n","        # update learning rate\n","        global_steps.assign_add(1)\n","        if global_steps < warmup_steps:# and not TRAIN_TRANSFER:\n","            lr = global_steps / warmup_steps * TRAIN_LR_INIT\n","        else:\n","            lr = TRAIN_LR_END + 0.5 * (TRAIN_LR_INIT - TRAIN_LR_END)*(\n","                (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi)))\n","        optimizer.lr.assign(lr.numpy())\n","\n","    return global_steps.numpy(), optimizer.lr.numpy(), giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()\n","\n","\n","\n","def validate_step(image_data, target):\n","    with tf.GradientTape() as tape:\n","        pred_result = yolo(image_data, training=False)\n","        giou_loss=conf_loss=prob_loss=0\n","\n","        # optimizing process\n","        for i in range(3):\n","            conv, pred = pred_result[i*2], pred_result[i*2+1]\n","            loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=TRAIN_CLASSES)\n","            giou_loss += loss_items[0]\n","            conf_loss += loss_items[1]\n","            prob_loss += loss_items[2]\n","\n","        total_loss = giou_loss + conf_loss + prob_loss\n","        \n","    return giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()\n","\n","\n","best_val_loss = 1000 # should be large at start\n","\n","\n","for epoch in range(TRAIN_EPOCHS):\n","    for image_data, target in trainset:\n","        results = train_step(image_data, target)\n","        cur_step = results[0]%steps_per_epoch\n","        print(\"epoch:{:2.0f} step:{:5.0f}/{}, lr:{:.6f}, giou_loss:{:7.2f}, conf_loss:{:7.2f}, prob_loss:{:7.2f}, total_loss:{:7.2f}\"\n","                  .format(epoch, cur_step, steps_per_epoch, results[1], results[2], results[3], results[4], results[5]))\n","        \n","    count, giou_val, conf_val, prob_val, total_val = 0., 0, 0, 0, 0\n","    for image_data, target in testset:\n","        results = validate_step(image_data, target)\n","        count += 1\n","        giou_val += results[0]\n","        conf_val += results[1]\n","        prob_val += results[2]\n","        total_val += results[3]\n","\n","        \n","    print(\"\\n\\ngiou_val_loss:{:7.2f}, conf_val_loss:{:7.2f}, prob_val_loss:{:7.2f}, total_val_loss:{:7.2f}\\n\\n\".\n","          format(giou_val/count, conf_val/count, prob_val/count, total_val/count))\n","\n","    if save_best_only and best_val_loss > total_val/count: \n","        yolo.save_weights(\"./checkpoints/yolov3_custom\")\n","        best_val_loss = total_val/count\n","\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-3e306aeeb681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTRAIN_FROM_CHECKPOINT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FROM_CHECKPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/yolov3_custom"]}]},{"cell_type":"markdown","metadata":{"id":"4NoPYfFSXlWw"},"source":["test"]},{"cell_type":"code","metadata":{"id":"MKS9tanWXjI4"},"source":["\n","import os\n","import cv2\n","import numpy as np\n","import random\n","import time\n","import tensorflow as tf\n","from google.colab.patches import cv2_imshow\n","\n","\n","YOLO_INPUT_SIZE = 416\n","input_size=YOLO_INPUT_SIZE\n","\n","ID = random.randint(0, 200)\n","\n","image_path =\"./racoon_test_images/000022.jpg\" #image_info[0]\n","\n","yolo = Create_Yolov3(input_size=input_size, CLASSES=TRAIN_CLASSES)\n","yolo.load_weights(\"./checkpoints/yolov3_custom\")\n","\n","\n","\n","img = detect_image(yolo, image_path, \"\", input_size=input_size, show=True, CLASSES=TRAIN_CLASSES, rectangle_colors=(255,0,0))\n","cv2_imshow(img)"],"execution_count":null,"outputs":[]}]}