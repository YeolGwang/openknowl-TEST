{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20. yolov3_inference.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Z2mfDfIDx_cK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659834536608,"user_tz":-540,"elapsed":17685,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"741b0805-69ee-4b73-9795-ef68402a8910"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Km52U82G2MHm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659834944472,"user_tz":-540,"elapsed":9,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"05146722-b606-450c-bfcb-4af13c984ea8"},"source":["cd /content/drive/MyDrive/openknowl/codes/model_data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/openknowl/codes/model_data\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nVMTiSjY2e1J","executionInfo":{"status":"ok","timestamp":1659834946361,"user_tz":-540,"elapsed":264,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"ef67e8d8-dfed-44d4-8b3b-58655ad6f94d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mmodel_data\u001b[0m/\n"]}]},{"cell_type":"code","metadata":{"id":"OT7pqkJKKBPs"},"source":["import os\n","import cv2\n","import time\n","import random\n","import colorsys\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, ZeroPadding2D, BatchNormalization\n","from tensorflow.keras.regularizers import l2\n","from google.colab.patches import cv2_imshow\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBbhS_qJv4Tn"},"source":["config 설정\n","\n"]},{"cell_type":"code","metadata":{"id":"En7e4Wz_VyaC"},"source":["# YOLO options\n","YOLO_DARKNET_WEIGHTS        = \"./model_data/yolov3.weights\"\n","YOLO_COCO_CLASSES           = \"./model_data/coco.names\"\n","YOLO_STRIDES                = [8, 16, 32]\n","YOLO_IOU_LOSS_THRESH        = 0.5\n","#YOLO_ANCHOR_PER_SCALE       = 3\n","#YOLO_MAX_BBOX_PER_SCALE     = 100\n","YOLO_INPUT_SIZE             = 416\n","YOLO_ANCHORS                = [[[10,  13], [16,   30], [33,   23]],\n","                               [[30,  61], [62,   45], [59,  119]],\n","                               [[116, 90], [156, 198], [373, 326]]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mG_11tKhjJrG"},"source":["def read_class_names(class_file_name):\n","    # loads class name from a file\n","    names = {}\n","    with open(class_file_name, 'r') as data:\n","        for ID, name in enumerate(data):\n","            names[ID] = name.strip('\\n')\n","    return names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["class 확인"],"metadata":{"id":"r7vtrIhu-nPy"}},{"cell_type":"code","metadata":{"id":"LWJ6MUXcjOgG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659834960299,"user_tz":-540,"elapsed":386,"user":{"displayName":"열광","userId":"00159998398443914453"}},"outputId":"e20552fa-b42f-440a-a454-e401b02f18fe"},"source":["NUM_CLASS = read_class_names(YOLO_COCO_CLASSES)\n","NUM_CLASS"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'person',\n"," 1: 'bicycle',\n"," 2: 'car',\n"," 3: 'motorbike',\n"," 4: 'aeroplane',\n"," 5: 'bus',\n"," 6: 'train',\n"," 7: 'truck',\n"," 8: 'boat',\n"," 9: 'traffic light',\n"," 10: 'fire hydrant',\n"," 11: 'stop sign',\n"," 12: 'parking meter',\n"," 13: 'bench',\n"," 14: 'bird',\n"," 15: 'cat',\n"," 16: 'dog',\n"," 17: 'horse',\n"," 18: 'sheep',\n"," 19: 'cow',\n"," 20: 'elephant',\n"," 21: 'bear',\n"," 22: 'zebra',\n"," 23: 'giraffe',\n"," 24: 'backpack',\n"," 25: 'umbrella',\n"," 26: 'handbag',\n"," 27: 'tie',\n"," 28: 'suitcase',\n"," 29: 'frisbee',\n"," 30: 'skis',\n"," 31: 'snowboard',\n"," 32: 'sports ball',\n"," 33: 'kite',\n"," 34: 'baseball bat',\n"," 35: 'baseball glove',\n"," 36: 'skateboard',\n"," 37: 'surfboard',\n"," 38: 'tennis racket',\n"," 39: 'bottle',\n"," 40: 'wine glass',\n"," 41: 'cup',\n"," 42: 'fork',\n"," 43: 'knife',\n"," 44: 'spoon',\n"," 45: 'bowl',\n"," 46: 'banana',\n"," 47: 'apple',\n"," 48: 'sandwich',\n"," 49: 'orange',\n"," 50: 'broccoli',\n"," 51: 'carrot',\n"," 52: 'hot dog',\n"," 53: 'pizza',\n"," 54: 'donut',\n"," 55: 'cake',\n"," 56: 'chair',\n"," 57: 'sofa',\n"," 58: 'pottedplant',\n"," 59: 'bed',\n"," 60: 'diningtable',\n"," 61: 'toilet',\n"," 62: 'tvmonitor',\n"," 63: 'laptop',\n"," 64: 'mouse',\n"," 65: 'remote',\n"," 66: 'keyboard',\n"," 67: 'cell phone',\n"," 68: 'microwave',\n"," 69: 'oven',\n"," 70: 'toaster',\n"," 71: 'sink',\n"," 72: 'refrigerator',\n"," 73: 'book',\n"," 74: 'clock',\n"," 75: 'vase',\n"," 76: 'scissors',\n"," 77: 'teddy bear',\n"," 78: 'hair drier',\n"," 79: 'toothbrush'}"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["전처리 : 비율에 맞게 크기 조정(resize) 및 Normalization"],"metadata":{"id":"nOVCvS9A-qQD"}},{"cell_type":"code","metadata":{"id":"4rjGhQcZi9WN"},"source":["def image_preprocess(image, target_size):\n","    ih, iw    = target_size\n","    h,  w, _  = image.shape\n","\n","    scale = min(iw/w, ih/h)\n","    nw, nh  = int(scale * w), int(scale * h)\n","    image_resized = cv2.resize(image, (nw, nh))\n","\n","    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n","    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n","    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n","    image_paded = image_paded / 255.\n","\n","    return image_paded"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Resize 예제"],"metadata":{"id":"1MHKoLUR_CrW"}},{"cell_type":"code","metadata":{"id":"3tJU5fl2lNMm"},"source":["image_path =\"./airplane.jpg\"\n","input_size = 416\n","original_image      = cv2.imread(image_path)\n","\n","\n","print(\"original:\",original_image.shape)\n","\n","image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n","print(\"resized:\",image_data.shape)\n","cv2_imshow(image_data*255)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bound box(Bbox) 그리기"],"metadata":{"id":"rWPj4FN--4AU"}},{"cell_type":"code","metadata":{"id":"I6j167A4mcqb"},"source":["def draw_bbox(image, bboxes, CLASSES=YOLO_COCO_CLASSES, show_label=True, show_confidence = True, Text_colors=(0,0,0), rectangle_colors=''):   \n","    NUM_CLASS = read_class_names(CLASSES)\n","    num_classes = len(NUM_CLASS)\n","    image_h, image_w, _ = image.shape\n","\n","    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n","    \n","    #print(\"hsv_tuples\", hsv_tuples)\n","    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n","    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n","\n","    random.seed(0)\n","    random.shuffle(colors)\n","    random.seed(None)\n","\n","    for i, bbox in enumerate(bboxes):\n","        coor = np.array(bbox[:4], dtype=np.int32)\n","        score = bbox[4]\n","        class_ind = int(bbox[5])\n","        bbox_color = rectangle_colors if rectangle_colors != ''else colors[class_ind]\n","        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n","        if bbox_thick < 1: bbox_thick = 1\n","        #print(image_h, image_w, bbox_thick)\n","        fontScale = 0.75 * bbox_thick\n","        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n","\n","        # put object rectangle\n","        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n","\n","        if show_label:\n","            # get text label\n","            score_str = f' {score:.2f}' if show_confidence else '' \n","            label = f'{NUM_CLASS[class_ind]}' + score_str\n","\n","            # get text size\n","            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n","                                                                  fontScale, thickness=bbox_thick)\n","            # put filled text rectangle\n","            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n","\n","            # put text above rectangle\n","            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n","                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n","\n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bbox 그리기 예제"],"metadata":{"id":"f73i4z8q--Yw"}},{"cell_type":"code","metadata":{"id":"6QTNSsZKmlSy"},"source":["# image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, rectangle_colors=rectangle_colors)\n","\n","bboxes = [[120,100,700,300,1.0, 4]]\n","\n","img = draw_bbox(original_image, bboxes, YOLO_COCO_CLASSES, show_label=True, show_confidence = True,rectangle_colors='' )\n","\n","cv2_imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IoU (intersection over union) 지표계산"],"metadata":{"id":"0YgG-bWN_Fzd"}},{"cell_type":"code","metadata":{"id":"-aVztBcnp7Ql"},"source":["def bboxes_iou(boxes1, boxes2):\n","    boxes1 = np.array(boxes1)\n","    boxes2 = np.array(boxes2)\n","\n","\n","    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n","    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n","\n","    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n","    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n","\n","    inter_section = np.maximum(right_down - left_up, 0.0)\n","    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n","    union_area    = boxes1_area + boxes2_area - inter_area\n","    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n","\n","    return ious"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IoU 계산 예제"],"metadata":{"id":"GxSlKnFd_Oc_"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHfvqf6bq8YF","executionInfo":{"status":"ok","timestamp":1617519733807,"user_tz":-540,"elapsed":613,"user":{"displayName":"modric e","photoUrl":"","userId":"13359243005730783631"}},"outputId":"a7ee302b-a1bd-40af-c60d-214c1a4e974d"},"source":["boxes1=[100,100,200,200]  # 10000\n","boxes2=[120,120,220,220]  # 10000     - 6400 = 6400/(20000-6400)\n","\n","print(bboxes_iou(boxes1, boxes2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.47058823529411764\n"],"name":"stdout"}]},{"cell_type":"markdown","source":["Non maximum algorithm(NMS)"],"metadata":{"id":"cyVZeWc1_Ray"}},{"cell_type":"code","metadata":{"id":"oKrkfTcpqNBS"},"source":["def nms(bboxes, iou_threshold):\n","    \"\"\"\n","    bboxes: (xmin, ymin, xmax, ymax, score, class)\n","    \"\"\"\n","    classes_in_img = list(set(bboxes[:, 5]))\n","    best_bboxes = []\n","\n","    for cls in classes_in_img:\n","        cls_mask = (bboxes[:, 5] == cls)\n","        cls_bboxes = bboxes[cls_mask]\n","        # Process 1: Determine whether the number of bounding boxes is greater than 0 \n","        while len(cls_bboxes) > 0:\n","            # Process 2: Select the bounding box with the highest score according to socre order A\n","            max_ind = np.argmax(cls_bboxes[:, 4])\n","            best_bbox = cls_bboxes[max_ind]\n","            best_bboxes.append(best_bbox)\n","            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n","            # Process 3: Calculate this bounding box A and\n","            # Remain all iou of the bounding box and remove those bounding boxes whose iou value is higher than the threshold \n","            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n","            weight = np.ones((len(iou),), dtype=np.float32)\n","\n","            iou_mask = iou > iou_threshold\n","            weight[iou_mask] = 0.0\n","\n","            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n","            score_mask = cls_bboxes[:, 4] > 0.\n","            cls_bboxes = cls_bboxes[score_mask]\n","\n","    return best_bboxes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NMS 예제"],"metadata":{"id":"5iGsp2W6_Zlr"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j0mk55rrq6bN","executionInfo":{"status":"ok","timestamp":1617520109309,"user_tz":-540,"elapsed":682,"user":{"displayName":"modric e","photoUrl":"","userId":"13359243005730783631"}},"outputId":"9ddefc16-37f5-4cbb-f472-2985d6b968b3"},"source":["bboxes = [\n","          [10,10,100,100, 0.9, 1],\n","           [20,20,120,120, 0.8, 1],\n","           [50,50,150,150, 0.7, 1],\n","           [10,10,120,120, 0.6, 2],\n","           [30,30,140,140, 0.9, 2],\n","          ]\n","\n","bboxes=  np.array(bboxes)\n","best_bboxes = nms(bboxes, iou_threshold=0.45)\n","\n","best_bboxes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([ 10. ,  10. , 100. , 100. ,   0.9,   1. ]),\n"," array([ 50. ,  50. , 150. , 150. ,   0.7,   1. ]),\n"," array([ 30. ,  30. , 140. , 140. ,   0.9,   2. ])]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","source":["후처리 : invalid bbox 제거"],"metadata":{"id":"XW8LE8YL_bqS"}},{"cell_type":"code","metadata":{"id":"q9D-yMuOqYr_"},"source":["def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):\n","    valid_scale=[0, np.inf]\n","    pred_bbox = np.array(pred_bbox)\n","\n","    pred_xywh = pred_bbox[:, 0:4]\n","    pred_conf = pred_bbox[:, 4]\n","    pred_prob = pred_bbox[:, 5:]\n","\n","    # 1. (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n","    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n","                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n","    \n","    # 2. (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n","    org_h, org_w = original_image.shape[:2]\n","    resize_ratio = min(input_size / org_w, input_size / org_h)\n","\n","    dw = (input_size - resize_ratio * org_w) / 2\n","    dh = (input_size - resize_ratio * org_h) / 2\n","\n","    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n","    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n","\n","    # 3. clip some boxes those are out of range\n","    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n","                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n","    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n","    pred_coor[invalid_mask] = 0\n","\n","    # 4. discard some invalid boxes\n","    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n","    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n","\n","    # 5. discard boxes with low scores\n","    classes = np.argmax(pred_prob, axis=-1)\n","    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n","    score_mask = scores > score_threshold\n","    mask = np.logical_and(scale_mask, score_mask)\n","    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n","\n","    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["후처리 예제"],"metadata":{"id":"Y1f-R6Hy_pvB"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xx6REtwj0yo7","executionInfo":{"status":"ok","timestamp":1617520736880,"user_tz":-540,"elapsed":532,"user":{"displayName":"modric e","photoUrl":"","userId":"13359243005730783631"}},"outputId":"46763bdd-d046-40ba-e0bd-7586f27f9433"},"source":["pred_bbox = [\n","           [10,10,100,100, 0.9, 0,3,0.7],\n","           [20,20,120,120, 0.8, 0,5,0.5],\n","           [50,50,150,150, 0.7, 0,4,0.6],\n","           [10,10,120,120, 0.6, 0,3,0.7],\n","           [30,30,140,140, 0.9, 0,9,0.1],\n","          ]\n","\n","pred_bbox=  np.array(pred_bbox)\n","results = postprocess_boxes(pred_bbox,original_image,input_size=416,score_threshold=0.3)\n","results[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  0.        ,   0.        , 219.65144231,  72.65144231,\n","         2.8       ,   1.        ])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","source":["Image detection 함수"],"metadata":{"id":"CFcdldXb_tII"}},{"cell_type":"code","metadata":{"id":"-AdiKSB0uvpJ"},"source":["def detect_image(YoloV3, image_path, output_path, input_size=416, show=False, CLASSES=YOLO_COCO_CLASSES, score_threshold=0.3, iou_threshold=0.45, rectangle_colors=''):  #iou_threshold=0.45\n","    original_image      = cv2.imread(image_path)\n","    # original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","    # original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","\n","    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n","    image_data = tf.expand_dims(image_data, 0)\n","\n","    pred_bbox = YoloV3.predict(image_data)\n","    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n","    pred_bbox = tf.concat(pred_bbox, axis=0)\n","    \n","    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n","    bboxes = nms(bboxes, iou_threshold)\n","\n","    image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, rectangle_colors=rectangle_colors)\n","    #cv2.imwrite('./output2.jpg', image)\n","    return image\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["video detection 함수"],"metadata":{"id":"M25e3C2F_xoU"}},{"cell_type":"code","metadata":{"id":"1kRCLzvOM13D"},"source":["def detect_video(Yolo, video_path, output_path, input_size=416, show=False, CLASSES=YOLO_COCO_CLASSES, score_threshold=0.3, iou_threshold=0.45, rectangle_colors=''):\n","    times, times_2 = [], []\n","    vid = cv2.VideoCapture(video_path)\n","\n","    # by default VideoCapture returns float instead of int\n","    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(vid.get(cv2.CAP_PROP_FPS))\n","    codec = cv2.VideoWriter_fourcc(*'XVID')\n","    out = cv2.VideoWriter(output_path, codec, fps, (width, height)) # output_path must be .mp4\n","\n","    while True:\n","        _, img = vid.read()\n","\n","        try:\n","            original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","        except:\n","            break\n","\n","        image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n","        image_data = image_data[np.newaxis, ...].astype(np.float32)\n","\n","        t1 = time.time()\n","        \n","        pred_bbox = Yolo.predict(image_data)\n","         \n","        t2 = time.time()\n","        \n","        pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n","        pred_bbox = tf.concat(pred_bbox, axis=0)\n","\n","\n","        print(\"pred bbox:\",pred_bbox)\n","\n","        bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n","\n","        print(\"post bbox:\",pred_bbox)\n","\n","        bboxes = nms(bboxes, iou_threshold)\n","        \n","        image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, rectangle_colors=rectangle_colors)\n","\n","        t3 = time.time()\n","        times.append(t2-t1)\n","        times_2.append(t3-t1)\n","        \n","        times = times[-20:]\n","        times_2 = times_2[-20:]\n","\n","        ms = sum(times)/len(times)*1000\n","        fps = 1000 / ms\n","        fps2 = 1000 / (sum(times_2)/len(times_2)*1000)\n","        \n","        image = cv2.putText(image, \"Time: {:.1f}FPS\".format(fps), (0, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n","        # CreateXMLfile(\"XML_Detections\", str(int(time.time())), original_image, bboxes, read_class_names(CLASSES))\n","        \n","        print(\"Time: {:.2f}ms, Detection FPS: {:.1f}, total FPS: {:.1f}\".format(ms, fps, fps2))\n","        if output_path != '': out.write(image)\n","\n","        if show:\n","            cv2.imshow('output', image)\n","            if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n","                cv2.destroyAllWindows()\n","                break\n","\n","    cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wIMyNoGSYKUf"},"source":["YOLO V3 Model"]},{"cell_type":"code","metadata":{"id":"fp0bysZfWGCh"},"source":["\n","STRIDES         = np.array(YOLO_STRIDES)\n","ANCHORS         = (np.array(YOLO_ANCHORS).T/STRIDES).T\n","IOU_LOSS_THRESH = YOLO_IOU_LOSS_THRESH\n","\n","\n","# The meaning of setting layer.trainable = False is to freeze the layer its internal state will not change during training: its trainable weights will not be updated during fit()\n","\n","\n","# Not freeze &  \n","\n","\n","\n","# freeze, infrence = false / flase \n","# freeze, training = false / true\n","# not freeze, inference = true / false\n","# not freeze, training = true / true\n","\n","\n","# batch layer에서 mean/var를 training 하고자 할 때, freeze 안된 상태여야함(self.trainable = True )\n","# 동시에 inference mode 여야함 (training  = true  )\n","\n","class BatchNormalization(BatchNormalization):\n","    # \"Frozen state\" and \"inference mode\" are two separate concepts.\n","    # `layer.trainable = False` is to freeze the layer, so the layer will use\n","    # stored moving `var` and `mean` in the \"inference mode\", and both `gama`\n","    # and `beta` will not be updated !\n","    def call(self, x, training=False):\n","        if not training:\n","            training = tf.constant(False)\n","        training = tf.logical_and(training, self.trainable)\n","        return super().call(x, training)\n","\n","\n","\n","def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):\n","    if downsample:\n","        input_layer = ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n","        padding = 'valid'\n","        strides = 2\n","    else:\n","        strides = 1\n","        padding = 'same'\n","\n","    conv = Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides,\n","                  padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),\n","                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n","                  bias_initializer=tf.constant_initializer(0.))(input_layer)\n","    if bn:\n","        conv = BatchNormalization()(conv)\n","    if activate == True:\n","        conv = LeakyReLU(alpha=0.1)(conv)\n","\n","    return conv\n","\n","def residual_block(input_layer, input_channel, filter_num1, filter_num2):\n","    short_cut = input_layer\n","    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))\n","    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))\n","\n","    residual_output = short_cut + conv\n","    return residual_output\n","\n","def upsample(input_layer):\n","    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')\n","\n","\n","def darknet53(input_data):\n","    input_data = convolutional(input_data, (3, 3,  3,  32))\n","    input_data = convolutional(input_data, (3, 3, 32,  64), downsample=True)\n","\n","    for i in range(1):\n","        input_data = residual_block(input_data,  64,  32, 64)\n","\n","    input_data = convolutional(input_data, (3, 3,  64, 128), downsample=True)\n","\n","    for i in range(2):\n","        input_data = residual_block(input_data, 128,  64, 128)\n","\n","    input_data = convolutional(input_data, (3, 3, 128, 256), downsample=True)\n","\n","    for i in range(8):\n","        input_data = residual_block(input_data, 256, 128, 256)\n","\n","    route_1 = input_data\n","    input_data = convolutional(input_data, (3, 3, 256, 512), downsample=True)\n","\n","    for i in range(8):\n","        input_data = residual_block(input_data, 512, 256, 512)\n","\n","    route_2 = input_data\n","    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True)\n","\n","    for i in range(4):\n","        input_data = residual_block(input_data, 1024, 512, 1024)\n","\n","    return route_1, route_2, input_data\n","\n","def YOLOv3(input_layer, NUM_CLASS):\n","    # the Darknet-53 네트워크로부터 3개의 branch\n","    route_1, route_2, conv = darknet53(input_layer)\n","\n","    # 5 회 convolution 연산\n","    conv = convolutional(conv, (1, 1, 1024,  512))\n","    conv = convolutional(conv, (3, 3,  512, 1024))\n","    conv = convolutional(conv, (1, 1, 1024,  512))\n","    conv = convolutional(conv, (3, 3,  512, 1024))\n","    conv = convolutional(conv, (1, 1, 1024,  512))\n","    \n","    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))\n","    \n","    # conv_lbbox is used to predict large-sized objects , Shape = [None, 13, 13, 255] \n","    conv_lbbox = convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n","\n","    conv = convolutional(conv, (1, 1,  512,  256))\n","    # upsample here uses the nearest neighbor interpolation method, which has the advantage that the\n","    # upsampling process does not need to learn, thereby reducing the network parameter  \n","    conv = upsample(conv)\n","\n","    conv = tf.concat([conv, route_2], axis=-1)\n","    conv = convolutional(conv, (1, 1, 768, 256))\n","    conv = convolutional(conv, (3, 3, 256, 512))\n","    conv = convolutional(conv, (1, 1, 512, 256))\n","    conv = convolutional(conv, (3, 3, 256, 512))\n","    conv = convolutional(conv, (1, 1, 512, 256))\n","    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))\n","\n","    # conv_mbbox is used to predict medium-sized objects, shape = [None, 26, 26, 255]\n","    conv_mbbox = convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n","\n","    conv = convolutional(conv, (1, 1, 256, 128))\n","    conv = upsample(conv)\n","\n","    conv = tf.concat([conv, route_1], axis=-1)\n","    conv = convolutional(conv, (1, 1, 384, 128))\n","    conv = convolutional(conv, (3, 3, 128, 256))\n","    conv = convolutional(conv, (1, 1, 256, 128))\n","    conv = convolutional(conv, (3, 3, 128, 256))\n","    conv = convolutional(conv, (1, 1, 256, 128))\n","    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))\n","    \n","    # conv_sbbox is used to predict small size objects, shape = [None, 52, 52, 255]\n","    conv_sbbox = convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)\n","        \n","    return [conv_sbbox, conv_mbbox, conv_lbbox]\n","\n","def Create_Yolov3(input_size=416, channels=3, training=False, CLASSES=YOLO_COCO_CLASSES):\n","    NUM_CLASS = len(read_class_names(CLASSES))\n","    input_layer  = Input([input_size, input_size, channels])\n","\n","    conv_tensors = YOLOv3(input_layer, NUM_CLASS)\n","\n","    output_tensors = []\n","    for i, conv_tensor in enumerate(conv_tensors):\n","        pred_tensor = decode(conv_tensor, NUM_CLASS, i)\n","        if training: output_tensors.append(conv_tensor)\n","        output_tensors.append(pred_tensor)\n","\n","    YoloV3 = tf.keras.Model(input_layer, output_tensors)\n","    return YoloV3\n","\n","def decode(conv_output, NUM_CLASS, i=0):\n","    # where i = 0, 1 or 2 to correspond to the three grid scales  \n","    conv_shape       = tf.shape(conv_output)\n","    batch_size       = conv_shape[0]\n","    output_size      = conv_shape[1]\n","\n","    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n","\n","    conv_raw_dxdy = conv_output[:, :, :, :, 0:2] # offset of center position     \n","    conv_raw_dwdh = conv_output[:, :, :, :, 2:4] # Prediction box length and width offset\n","    conv_raw_conf = conv_output[:, :, :, :, 4:5] # confidence of the prediction box\n","    conv_raw_prob = conv_output[:, :, :, :, 5: ] # category probability of the prediction box \n","\n","    # next need Draw the grid. Where output_size is equal to 13, 26 or 52  \n","    y = tf.range(output_size, dtype=tf.int32)\n","    y = tf.expand_dims(y, -1)\n","    y = tf.tile(y, [1, output_size])\n","    x = tf.range(output_size,dtype=tf.int32)\n","    x = tf.expand_dims(x, 0)\n","    x = tf.tile(x, [output_size, 1])\n","\n","    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n","    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\n","    xy_grid = tf.cast(xy_grid, tf.float32)\n","\n","    # Calculate the center position of the prediction box:\n","    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n","    # Calculate the length and width of the prediction box:\n","    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n","\n","    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n","    pred_conf = tf.sigmoid(conv_raw_conf) # object box calculates the predicted confidence\n","    pred_prob = tf.sigmoid(conv_raw_prob) # calculating the predicted probability category box object\n","\n","    # calculating the predicted probability category box object\n","    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["yolov3.weights 를 load"],"metadata":{"id":"oNYjfs-x_7Eg"}},{"cell_type":"code","metadata":{"id":"_kyvsFH0-w_l"},"source":["def load_yolo_weights(model, weights_file):\n","    print('load yolo weight called')\n","    tf.keras.backend.clear_session() # used to reset layer names\n","    # load Darknet original weights to TensorFlow model\n"," \n","    range1 = 75 \n","    range2 = [58, 66, 74] \n","    \n","    with open(weights_file, 'rb') as wf:\n","        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n","\n","        j = 0\n","        for i in range(range1):\n","            if i > 0:\n","                conv_layer_name = 'conv2d_%d' %i\n","            else:\n","                conv_layer_name = 'conv2d'\n","                \n","            if j > 0:\n","                bn_layer_name = 'batch_normalization_%d' %j\n","            else:\n","                bn_layer_name = 'batch_normalization'\n","            \n","            conv_layer = model.get_layer(conv_layer_name)\n","            filters = conv_layer.filters\n","            k_size = conv_layer.kernel_size[0]\n","            in_dim = conv_layer.input_shape[-1]\n","\n","            if i not in range2:\n","               # darknet weights: [beta, gamma, mean, variance]\n","               bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n","               # tf weights: [gamma, beta, mean, variance]\n","               bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n","               bn_layer = model.get_layer(bn_layer_name)\n","               j += 1\n","            else:\n","                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n","\n","            #darknet shape (out_dim, in_dim, height, width)\n","            conv_shape = (filters, in_dim, k_size, k_size)\n","            conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n","            #tf shape (height, width, in_dim, out_dim)\n","            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n","\n","            if i not in range2:\n","                conv_layer.set_weights([conv_weights])\n","                bn_layer.set_weights(bn_weights)\n","                #print('called')\n","            else:\n","                conv_layer.set_weights([conv_weights, conv_bias])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qW9f0PINdGCb","executionInfo":{"status":"ok","timestamp":1612079984024,"user_tz":-540,"elapsed":757,"user":{"displayName":"modric e","photoUrl":"","userId":"13359243005730783631"}},"outputId":"d3384ce3-07a6-43e1-c952-909710c66c79"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/LearningSpoons CV'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"_xBOy61QkybS"},"source":["input_size=YOLO_INPUT_SIZE\n","image_path =\"./airplane.jpg\" \n","\n","#image_path =\"./racoon_test_images/000022.jpg\"\n","\n","yolo = Create_Yolov3(input_size=input_size, CLASSES=YOLO_COCO_CLASSES)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2r4navqb9xl"},"source":["load_yolo_weights(yolo, \"./model_data/yolov3.weights\") # use Darknet weights\n","img = detect_image(yolo, image_path, \"\", input_size=input_size, show=True, CLASSES=YOLO_COCO_CLASSES, rectangle_colors=(255,0,0))\n","#cv2_imshow(img)"],"execution_count":null,"outputs":[]}]}