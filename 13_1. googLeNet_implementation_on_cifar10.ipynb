{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"13_1. googLeNet_implementation_on_cifar10.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"id":"5ZiAVJEcNVe1"},"source":["from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Activation, add, Flatten, AveragePooling2D, concatenate\n","from keras.models import Model\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","import tensorflow as tf\n","from keras.utils import np_utils\n","\n","width = 32\n","height = 32\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F1PIxfu-FJTd"},"source":["https://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3"]},{"cell_type":"markdown","source":["1. label 을 one-hot vector 로 변환"],"metadata":{"id":"bi9LC2i7j68A"}},{"cell_type":"code","metadata":{"id":"8w4-9oE4Ngaj"},"source":["num_classes = 10\n","y_train = np_utlls.to_categorical(y_train)\n","y_test = np_utlls.to_categorical(y_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. list slicing 으로  train image / validation image 분리 "],"metadata":{"id":"MekzudIwkMXs"}},{"cell_type":"code","metadata":{"id":"E-GhJpHuW02A"},"source":["valldation_images, valldation_labels = x_trin[:500], y_trin[:500]\n","train_images, train_labels = x_train[500:], y_train[500:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qCGvbXliD5Au","executionInfo":{"status":"ok","timestamp":1616309193117,"user_tz":-540,"elapsed":614,"user":{"displayName":"modric e","photoUrl":"","userId":"13359243005730783631"}},"outputId":"65acc92c-a6ba-4e25-c674-eba83cacd217"},"source":["train_images.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(49500, 32, 32, 3)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","source":["3. Inception module 정의"],"metadata":{"id":"l-bm0CEKkmc_"}},{"cell_type":"code","metadata":{"id":"gWFlCh3bHU2p"},"source":["def inception(x, filters): \n","    pre_layer = x\n","    \n","    f1,f2,f3,f4 = filters\n","\n","    # 1x1\n","    \n","    # 1x1 & 3x3\n","\n","    # 1x1 & 5x5\n","    \n","    # pooling & 1x1\n","\n","    # output = [None(batch_size), w,h,c], c 기준 concatenate (axis = -1 )\n","    concat = concatenate([conv1, conv2, conv3, max_pool], axis=-1)\n","    \n","    return concat\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. GoogleNet (Incepceion V1) 모델 정의"],"metadata":{"id":"Npd-N8XpktX1"}},{"cell_type":"code","metadata":{"id":"_BQRbgfkHbCV"},"source":["input_shape =  x_train[0].shape\n","inputs = Input(shape= input_shape )\n","\n","# conv랑 batch 사이에 max pooling 들어가야 하나, cifar 데이터에선 크기 너무 줄어들어서 뺐음\n","x = Conv2D(64, kernel_size=(7,7), strides=2, padding='same', activation='relu')(inputs)\n","x = BatchNormalization()(x)\n","x = Conv2D(192, kernel_size=(3,3), padding='same', activation='relu')(x)\n","x = BatchNormalization()(x) \n","\n","#inception (input, [#1x1conv, #3x3conv, #5x5conv, #1x1reduction] )\n","\n","# inception 3a\n","x= inception(x, [64 ,128 ,32 , 32 ])\n","# inception 3b\n","x= inception(x, [128 ,192 ,96 ,64 ])\n","x = MaxPooling2D(pool_size=(3,3),strides = 2, padding='same')(x)\n","\n","\n","# inception 4a\n","x= inception(x, [192 , 208 , 48 ,64 ])\n","\n","#aux1\n","aux1 = AveragePooling2D(pool_size=(5,5), strides = 3, padding= 'valid')(x)\n","aux1 = Conv2D(128, kernel_size = (1,1), padding='same', activation = 'relu')(aux1)\n","aux1 = Flatten()(aux1)\n","aux1 = Dense(512, activation ='relu')(aux1)\n","aux1 = Dense(10, activation = 'softmax')(aux1)\n","\n","# inception 4b\n","x= inception(x, [160 , 224  ,64  ,64 ])\n","# inception 4c\n","x= inception(x, [128 ,256  ,64  ,64 ])\n","# inception 4d\n","x= inception(x, [112 ,288  ,64  ,64 ])\n","\n","#aux2\n","aux2 = AveragePooling2D(pool_size=(5,5), strides = 3, padding= 'valid')(x)\n","aux2 = Conv2D(128, kernel_size = (1,1), padding='same', activation = 'relu')(aux2)\n","aux2 = Flatten()(aux2)\n","aux2 = Dense(832, activation ='relu')(aux2)\n","aux2 = Dense(10, activation = 'softmax')(aux2)\n","\n","\n","# inception 4e\n","x= inception(x, [256 ,320  ,128  ,128 ])\n","x = MaxPooling2D(pool_size=(3,3),strides = 2, padding='same')(x)\n","\n","\n","# inception 5a\n","x= inception(x, [256 ,320  ,128  ,128 ])\n","# inception 5b\n","x= inception(x, [384 ,384  ,128  ,128 ])\n","\n","x = AveragePooling2D(pool_size = (4,4), padding='valid')(x)\n","x = Dropout(0.4)(x)\n","\n","outputs = Dense( 10 , activation='softmax')(x)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nx3JQsDnBose"},"source":["model = Model( inputs = inputs, outputs=[aux1, aux2, outputs] )\n","\n","\n","# adam optimizer, categorical crossentropy, loss weight ratio (0.3,0.3,1.0), metric: accuracy\n","model.compile( optimizer = 'adam', loss='categorical_crossentropy', loss_weights=[0.3,0.3,1.0], metrics=['accuracy'] ) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLio7DUSOqPi"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. ImageDagaGenerator 로 normalization 및 augmentation 적용 (shear , zoom , horizontal flip 적용 )"],"metadata":{"id":"cMISniM5laMY"}},{"cell_type":"code","metadata":{"id":"hKIGz5YMWrKr"},"source":["train_datagen = imageDataGeneratop(rescale = 1./255, shear_range = 0.2, zoom_range =)\n","\n","validation_datagen ="],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. batch 32, epoch 10 으로 학습"],"metadata":{"id":"TpJlFNrel57z"}},{"cell_type":"code","metadata":{"id":"o73IuB0RXFoe"},"source":["history = model.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. train data, validation data 각각에 대해 마지막 layer의 loss 와 accuracy \n","\n","---\n","\n"],"metadata":{"id":"qa3FQbu5mCth"}},{"cell_type":"code","metadata":{"id":"nouTQsGTOR9i"},"source":["import matplotlib.pyplot as plt\n","\n","acc = \n","val_acc = \n","\n","loss =\n","val_loss = \n","\n","epochs_range = range(10)\n","\n","\n","# accuracy\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(  label='Training Accuracy')\n","plt.plot(  label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","# loss\n","plt.subplot(1, 2, 2)\n","plt.plot(  label='Training Loss')\n","plt.plot(   label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"execution_count":null,"outputs":[]}]}